{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f554df",
   "metadata": {
    "papermill": {
     "duration": 0.016971,
     "end_time": "2022-01-14T07:49:50.789985",
     "exception": false,
     "start_time": "2022-01-14T07:49:50.773014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Behavior Sequence Transformer\n",
    "\n",
    "using Transformer to capture the sequential signals underlying users' behavior sequences  \n",
    "\n",
    "### References\n",
    "- https://arxiv.org/pdf/1905.06874.pdf  \n",
    "- https://www.kaggle.com/laowingkin/netflix-movie-recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cc6464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:49:50.829811Z",
     "iopub.status.busy": "2022-01-14T07:49:50.828272Z",
     "iopub.status.idle": "2022-01-14T07:49:58.943094Z",
     "shell.execute_reply": "2022-01-14T07:49:58.943547Z",
     "shell.execute_reply.started": "2022-01-14T05:24:58.974534Z"
    },
    "papermill": {
     "duration": 8.136452,
     "end_time": "2022-01-14T07:49:58.943891",
     "exception": false,
     "start_time": "2022-01-14T07:49:50.807439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f47d21",
   "metadata": {
    "papermill": {
     "duration": 0.015676,
     "end_time": "2022-01-14T07:49:58.976067",
     "exception": false,
     "start_time": "2022-01-14T07:49:58.960391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4b1315",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-01-14T07:49:59.013746Z",
     "iopub.status.busy": "2022-01-14T07:49:59.012796Z",
     "iopub.status.idle": "2022-01-14T07:49:59.014443Z",
     "shell.execute_reply": "2022-01-14T07:49:59.014944Z"
    },
    "papermill": {
     "duration": 0.023069,
     "end_time": "2022-01-14T07:49:59.015087",
     "exception": false,
     "start_time": "2022-01-14T07:49:58.992018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['cust_id', 'rating', 'timestamp'])\n",
    "# df1['rating'] = df1['rating'].astype(float)\n",
    "# df1.head()\n",
    "\n",
    "# df2 = pd.read_csv('../input/netflix-prize-data/combined_data_2.txt', \n",
    "#                   header=None, names=['cust_id', 'rating', 'timestamp'])\n",
    "# df2['rating'] = df2['rating'].astype(float)\n",
    "# df1 = pd.concat([df1, df2])\n",
    "# print(df1.shape)\n",
    "# del df2\n",
    "\n",
    "# df3 = pd.read_csv('../input/netflix-prize-data/combined_data_3.txt', \n",
    "#                   header=None, names=['cust_id', 'rating', 'timestamp'])\n",
    "# df3['rating'] = df3['rating'].astype(float)\n",
    "# df1 = pd.concat([df1, df3])\n",
    "# print(df1.shape)\n",
    "# del df3\n",
    "\n",
    "# df4 = pd.read_csv('../input/netflix-prize-data/combined_data_4.txt', \n",
    "#                   header=None, names=['cust_id', 'rating', 'timestamp'])\n",
    "# df4['rating'] = df4['rating'].astype(float)\n",
    "# df1 = pd.concat([df1, df4])\n",
    "# print(df1.shape)\n",
    "# del df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34cf019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:49:59.052574Z",
     "iopub.status.busy": "2022-01-14T07:49:59.052015Z",
     "iopub.status.idle": "2022-01-14T07:49:59.055755Z",
     "shell.execute_reply": "2022-01-14T07:49:59.055228Z"
    },
    "papermill": {
     "duration": 0.024708,
     "end_time": "2022-01-14T07:49:59.055878",
     "exception": false,
     "start_time": "2022-01-14T07:49:59.031170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df1.to_parquet('/kaggle/working/ratings.parquet')\n",
    "# df1 = pd.read_parquet('../input/netflix-ratings/netflix_ratings.parquet')\n",
    "# df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67ad8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:49:59.093377Z",
     "iopub.status.busy": "2022-01-14T07:49:59.092561Z",
     "iopub.status.idle": "2022-01-14T07:49:59.094965Z",
     "shell.execute_reply": "2022-01-14T07:49:59.094544Z"
    },
    "papermill": {
     "duration": 0.022746,
     "end_time": "2022-01-14T07:49:59.095079",
     "exception": false,
     "start_time": "2022-01-14T07:49:59.072333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_nan = df1.loc[df1['rating'].isna()].reset_index().drop(['rating', 'timestamp'], axis=1)\n",
    "# df_nan['next'] = df_nan['index'].shift(-1).fillna(df1.index[-1]+1).astype(int)\n",
    "# df_nan['movie_id'] = df_nan['cust_id'].str[:-1].astype(int)\n",
    "# df_nan.drop('cust_id', axis=1, inplace=True)\n",
    "\n",
    "# movie_ids = np.full((1, df1.shape[0]), 0)\n",
    "# for i, j, k in tqdm(df_nan[['index', 'next', 'movie_id']].values):\n",
    "#     movie_ids[0, i+1:j] = k\n",
    "# df1['movie_id'] = movie_ids[0]\n",
    "\n",
    "# df2 = df1.loc[~df1['rating'].isna()]\n",
    "# del df1, df_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd785671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:49:59.133210Z",
     "iopub.status.busy": "2022-01-14T07:49:59.132336Z",
     "iopub.status.idle": "2022-01-14T07:50:10.965894Z",
     "shell.execute_reply": "2022-01-14T07:50:10.966316Z",
     "shell.execute_reply.started": "2022-01-14T05:30:52.574149Z"
    },
    "papermill": {
     "duration": 11.85526,
     "end_time": "2022-01-14T07:50:10.966494",
     "exception": false,
     "start_time": "2022-01-14T07:49:59.111234",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random_selection = np.random.rand(len(df2.index)) <= 0.5\n",
    "# df3 = df2[random_selection]\n",
    "# del df2\n",
    "# df_movie_summary = df3.groupby('movie_id')[['rating']].count()\n",
    "# df_user_summary = df3.groupby('cust_id')[['rating']].count()\n",
    "# drop_movie_list = df_movie_summary.loc[df_movie_summary['rating'] < 100].index\n",
    "# drop_cust_list = df_user_summary.loc[df_user_summary['rating'] < 100].index\n",
    "\n",
    "# df3 = df3[~df3['movie_id'].isin(drop_movie_list)]\n",
    "# df3 = df3[~df3['cust_id'].isin(drop_cust_list)]\n",
    "\n",
    "\n",
    "df3 = pd.read_parquet('../input/netflixratings/netflix_ratings_sampled01.parquet')\n",
    "df3['cust_id'] = df3['cust_id'].apply(lambda x: f'cust_{x}')\n",
    "df3['movie_id'] = df3['movie_id'].apply(lambda x: f'movie_{x}')\n",
    "# print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8e1df1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:50:11.706183Z",
     "iopub.status.busy": "2022-01-14T07:50:11.704978Z",
     "iopub.status.idle": "2022-01-14T07:50:32.813593Z",
     "shell.execute_reply": "2022-01-14T07:50:32.813091Z",
     "shell.execute_reply.started": "2022-01-14T05:31:01.824480Z"
    },
    "papermill": {
     "duration": 21.83038,
     "end_time": "2022-01-14T07:50:32.813755",
     "exception": false,
     "start_time": "2022-01-14T07:50:10.983375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_group = df3.sort_values('timestamp').groupby('cust_id')\n",
    "df3[['cust_id', 'movie_id']] = df3[['cust_id', 'movie_id']].astype('string')\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    'cust_id': list(df3.cust_id.unique()), \n",
    "    'movie_id': list(df3.movie_id.unique())\n",
    "}\n",
    "del df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e04889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:50:33.472708Z",
     "iopub.status.busy": "2022-01-14T07:50:33.471805Z",
     "iopub.status.idle": "2022-01-14T07:51:00.296457Z",
     "shell.execute_reply": "2022-01-14T07:51:00.296887Z",
     "shell.execute_reply.started": "2022-01-14T05:31:25.335256Z"
    },
    "papermill": {
     "duration": 27.465747,
     "end_time": "2022-01-14T07:51:00.297048",
     "exception": false,
     "start_time": "2022-01-14T07:50:32.831301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cust_id       0\n",
       "movie_ids     0\n",
       "ratings       0\n",
       "timestamps    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_data = pd.DataFrame(data={\n",
    "    'cust_id': list(ratings_group.groups.keys()), \n",
    "    'movie_ids': list(ratings_group.movie_id.apply(list)), \n",
    "    'ratings': list(ratings_group.rating.apply(list)),\n",
    "    'timestamps': list(ratings_group.timestamp.apply(list))\n",
    "})\n",
    "\n",
    "ratings_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840200b",
   "metadata": {
    "papermill": {
     "duration": 0.016679,
     "end_time": "2022-01-14T07:51:00.330707",
     "exception": false,
     "start_time": "2022-01-14T07:51:00.314028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc5451a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:51:00.380847Z",
     "iopub.status.busy": "2022-01-14T07:51:00.380008Z",
     "iopub.status.idle": "2022-01-14T07:51:32.819818Z",
     "shell.execute_reply": "2022-01-14T07:51:32.819317Z",
     "shell.execute_reply.started": "2022-01-14T05:33:36.569749Z"
    },
    "papermill": {
     "duration": 32.466379,
     "end_time": "2022-01-14T07:51:32.819975",
     "exception": false,
     "start_time": "2022-01-14T07:51:00.353596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length = 8\n",
    "step_size = 1\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    \n",
    "    while len(values[start_index:]) >= window_size:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "ratings_data.ratings = ratings_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.drop('timestamps', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44563e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:51:32.872088Z",
     "iopub.status.busy": "2022-01-14T07:51:32.871138Z",
     "iopub.status.idle": "2022-01-14T07:51:35.375609Z",
     "shell.execute_reply": "2022-01-14T07:51:35.374521Z",
     "shell.execute_reply.started": "2022-01-14T05:34:02.861000Z"
    },
    "papermill": {
     "duration": 2.537,
     "end_time": "2022-01-14T07:51:35.375776",
     "exception": false,
     "start_time": "2022-01-14T07:51:32.838776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_data_movies = ratings_data[['cust_id', 'movie_ids']]\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t.explode('movie_ids', ignore_index=True)\n",
    "ratings_data_rating = ratings_data[['ratings']]\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t.explode('ratings', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b715e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:51:37.205388Z",
     "iopub.status.busy": "2022-01-14T07:51:37.203872Z",
     "iopub.status.idle": "2022-01-14T07:51:39.305917Z",
     "shell.execute_reply": "2022-01-14T07:51:39.305348Z",
     "shell.execute_reply.started": "2022-01-14T05:34:27.462245Z"
    },
    "papermill": {
     "duration": 3.913634,
     "end_time": "2022-01-14T07:51:39.306063",
     "exception": false,
     "start_time": "2022-01-14T07:51:35.392429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del ratings_data\n",
    "ratings_data_transformed = \\\n",
    "\t\tpd.concat([ratings_data_movies, ratings_data_rating], axis=1).dropna()\n",
    "# del ratings_data_movies, ratings_data_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77e4e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:51:39.487004Z",
     "iopub.status.busy": "2022-01-14T07:51:39.459307Z",
     "iopub.status.idle": "2022-01-14T07:52:49.748268Z",
     "shell.execute_reply": "2022-01-14T07:52:49.747463Z",
     "shell.execute_reply.started": "2022-01-14T05:34:39.245043Z"
    },
    "papermill": {
     "duration": 70.425146,
     "end_time": "2022-01-14T07:52:49.748442",
     "exception": false,
     "start_time": "2022-01-14T07:51:39.323296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n"
     ]
    }
   ],
   "source": [
    "ratings_data_transformed.movie_ids = \\\n",
    "\t\tratings_data_transformed.movie_ids\\\n",
    "\t\t.apply(lambda x: ','.join(x))\n",
    "ratings_data_transformed.ratings = \\\n",
    "\t\tratings_data_transformed.ratings\\\n",
    "\t\t.apply(lambda x: ','.join([str(v) for v in x]))\n",
    "ratings_data_transformed.rename(\n",
    "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
    "    inplace=True,\n",
    ")\n",
    "print('here3')\n",
    "\n",
    "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.8\n",
    "train_data = ratings_data_transformed[random_selection]\n",
    "test_data = ratings_data_transformed[~random_selection]\n",
    "\n",
    "# train_data.to_parquet('/kaggle/working/train_data.parquet')\n",
    "# test_data.to_parquet('/kaggle/working/test_data.parquet')\n",
    "train_data.to_csv(\"/kaggle/working/train_data.csv\", index=False, sep=\"|\", header=False)\n",
    "test_data.to_csv(\"/kaggle/working/test_data.csv\", index=False, sep=\"|\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c62e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:52:49.800125Z",
     "iopub.status.busy": "2022-01-14T07:52:49.799417Z",
     "iopub.status.idle": "2022-01-14T07:52:52.734515Z",
     "shell.execute_reply": "2022-01-14T07:52:52.734026Z",
     "shell.execute_reply.started": "2022-01-14T05:35:51.581361Z"
    },
    "papermill": {
     "duration": 2.967209,
     "end_time": "2022-01-14T07:52:52.734707",
     "exception": false,
     "start_time": "2022-01-14T07:52:49.767498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 07:52:49.859979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:49.861398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:49.862169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:49.864371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-14 07:52:49.865894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:49.866600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:49.867359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:52.199664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:52.200459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:52.201151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-14 07:52:52.201747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15385 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "# CSV_HEADER = list(ratings_data_transformed.columns)\n",
    "CSV_HEADER = ['cust_id', 'sequence_movie_ids', 'sequence_ratings']\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        movie_ids_string = features['sequence_movie_ids']\n",
    "        sequence_movie_ids = tf.strings.split(movie_ids_string, ',').to_tensor()\n",
    "        \n",
    "        features['target_movie_id'] = sequence_movie_ids[:, -1]\n",
    "        features['sequence_movie_ids'] = sequence_movie_ids[:, :-1]\n",
    "        \n",
    "        ratings_string = features['sequence_ratings']\n",
    "        sequence_ratings = tf.strings.to_number(\n",
    "            tf.strings.split(ratings_string, ','), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "        \n",
    "        target = sequence_ratings[:, -1]\n",
    "        features['sequence_ratings'] = sequence_ratings[:, :-1]\n",
    "        return features, target\n",
    "    \n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path, \n",
    "        batch_size=batch_size, \n",
    "        column_names=CSV_HEADER, \n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim='|',\n",
    "        shuffle=shuffle\n",
    "    ).map(process)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_dataset_from_csv('train_data.csv', shuffle=True, batch_size=265)\n",
    "\n",
    "sequence_length = 8\n",
    "include_user_id = False\n",
    "hidden_units = [256, 128]\n",
    "dropout_rate = 0.1\n",
    "num_heads = 3\n",
    "\n",
    "def create_model_inputs():\n",
    "    return {\n",
    "        'cust_id': layers.Input(name='cust_id', shape=(1,), dtype=tf.string), \n",
    "        'sequence_movie_ids': layers.Input(name='sequence_movie_ids', \n",
    "                                           shape=(sequence_length - 1,), \n",
    "                                           dtype=tf.string),\n",
    "        'target_movie_id': layers.Input(name='target_movie_id', \n",
    "                                        shape=(1,), dtype=tf.string),\n",
    "        'sequence_ratings': layers.Input(name='sequence_ratings', \n",
    "                                         shape=(sequence_length - 1,), \n",
    "                                         dtype=tf.float32)\n",
    "    }\n",
    "\n",
    "inputs = create_model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eea704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:52:53.940703Z",
     "iopub.status.busy": "2022-01-14T07:52:53.939034Z",
     "iopub.status.idle": "2022-01-14T07:52:53.941290Z",
     "shell.execute_reply": "2022-01-14T07:52:53.941707Z",
     "shell.execute_reply.started": "2022-01-14T05:35:52.781976Z"
    },
    "papermill": {
     "duration": 1.189262,
     "end_time": "2022-01-14T07:52:53.941859",
     "exception": false,
     "start_time": "2022-01-14T07:52:52.752597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_input_features(inputs, include_user_id=True):\n",
    "    \n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "    other_feature_names = []\n",
    "    \n",
    "    if include_user_id:\n",
    "        other_feature_names.append('cust_id')\n",
    "        \n",
    "    for feature_name in other_feature_names:\n",
    "        # string input values -> integer indices\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        # nlp의 tokenizer와 비슷한 역할\n",
    "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, \n",
    "                           num_oov_indices=0)(inputs[feature_name])\n",
    "        \n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        embedding_encoder = layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f'{feature_name}_embedding'\n",
    "        )\n",
    "        \n",
    "        # convert the index values to embedding representation\n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "        \n",
    "    if len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "        \n",
    "    ###########################################################################\n",
    "    # movie_id                               \n",
    "    ###########################################################################\n",
    "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY['movie_id']\n",
    "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
    "    movie_index_lookup = StringLookup(\n",
    "        vocabulary=movie_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name='movie_index_lookup'\n",
    "    )\n",
    "    \n",
    "    movie_embedding_encoder = layers.Embedding(\n",
    "        input_dim=len(movie_vocabulary),\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name='movie_embedding'\n",
    "    )\n",
    "    \n",
    "    # define a function to encode a given movie id\n",
    "    def encode_movie(movie_id):\n",
    "        # string input -> integer indices\n",
    "        movie_idx = movie_index_lookup(movie_id)\n",
    "        encoded_movie = movie_embedding_encoder(movie_idx)\n",
    "        \n",
    "        return encoded_movie\n",
    "    \n",
    "    target_movie_id = inputs['target_movie_id']\n",
    "    encoded_target_movie = encode_movie(target_movie_id)\n",
    "    \n",
    "    sequence_movie_ids = inputs['sequence_movie_ids']\n",
    "    encoded_sequence_movies = encode_movie(sequence_movie_ids)\n",
    "    \n",
    "    ###########################################################################    \n",
    "    # position embedding\n",
    "    ###########################################################################\n",
    "    position_embedding_encoder = layers.Embedding(\n",
    "        input_dim=sequence_length, \n",
    "        output_dim=movie_embedding_dims, \n",
    "        name='poisition_embedding'\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encoded_positions = position_embedding_encoder(positions)\n",
    "    \n",
    "    ###########################################################################\n",
    "    # ratings\n",
    "    ###########################################################################\n",
    "    # shape (None, 7) -> shape (None, 7, 1)  \n",
    "    sequence_ratings = tf.expand_dims(inputs['sequence_ratings'], -1)\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    # inner product of movie id sequence + encoded position & sequence_rating\n",
    "    ###########################################################################\n",
    "    \n",
    "    # encoded_sequence_movies shape (7,114)\n",
    "    # encoded_positions shape (7,114)\n",
    "    # sequence_ratings shape (None, 7, 1)\n",
    "    # encoded_sequence_movies_with_position_and_rating shape (None, 7, 114)\n",
    "    \n",
    "    encoded_sequence_movies_with_position_and_rating = layers.Multiply()(\n",
    "        [(encoded_sequence_movies + encoded_positions), sequence_ratings]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # unstack -> (None, 7, 114)에서 114씩 encoded_movie로 분리됨\n",
    "    for encoded_movie in tf.unstack(encoded_sequence_movies_with_position_and_rating, axis=1):\n",
    "        # encoded_movie shape(None, 114) -> (None, 1, 114)\n",
    "        encoded_transformer_features.append(tf.expand_dims(encoded_movie, 1))\n",
    "    \n",
    "    encoded_transformer_features.append(encoded_target_movie)\n",
    "    encoded_transformer_features = layers.concatenate(encoded_transformer_features, axis=1)\n",
    "    \n",
    "    return encoded_transformer_features, encoded_other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e7738d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:52:54.004828Z",
     "iopub.status.busy": "2022-01-14T07:52:53.994546Z",
     "iopub.status.idle": "2022-01-14T07:52:54.279255Z",
     "shell.execute_reply": "2022-01-14T07:52:54.279710Z",
     "shell.execute_reply.started": "2022-01-14T05:35:52.801481Z"
    },
    "papermill": {
     "duration": 0.320469,
     "end_time": "2022-01-14T07:52:54.279887",
     "exception": false,
     "start_time": "2022-01-14T07:52:53.959418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer_features, other_features = encode_input_features(\n",
    "    inputs, include_user_id\n",
    ")\n",
    "\n",
    "# create a multi-headed attention layer\n",
    "# params: (target, source)\n",
    "attention_output = layers.MultiHeadAttention(\n",
    "    num_heads=num_heads, \n",
    "    key_dim=transformer_features.shape[2], \n",
    "    dropout=dropout_rate\n",
    ")(transformer_features, transformer_features)\n",
    "\n",
    "# transformer block\n",
    "attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "x1 = layers.Add()([transformer_features, attention_output])\n",
    "x1 = layers.LayerNormalization()(x1)\n",
    "x2 = layers.LeakyReLU()(x1)\n",
    "# 왜 shape[-1]인가\n",
    "x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "x2 = layers.Dropout(dropout_rate)(x2)\n",
    "transformer_features = layers.Add()([x1, x2])\n",
    "transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "features = layers.Flatten()(transformer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f97097d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:52:54.324922Z",
     "iopub.status.busy": "2022-01-14T07:52:54.323227Z",
     "iopub.status.idle": "2022-01-14T07:52:54.416380Z",
     "shell.execute_reply": "2022-01-14T07:52:54.416851Z",
     "shell.execute_reply.started": "2022-01-14T05:35:53.067356Z"
    },
    "papermill": {
     "duration": 0.119398,
     "end_time": "2022-01-14T07:52:54.417030",
     "exception": false,
     "start_time": "2022-01-14T07:52:54.297632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if other_features is not None:\n",
    "    features = layers.concatenate(\n",
    "        [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "    )\n",
    "\n",
    "for num_units in hidden_units:\n",
    "    features = layers.Dense(num_units)(features)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    features = layers.LeakyReLU()(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    \n",
    "outputs = layers.Dense(units=1)(features)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2fc145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:52:54.457877Z",
     "iopub.status.busy": "2022-01-14T07:52:54.457072Z",
     "iopub.status.idle": "2022-01-14T09:43:11.664494Z",
     "shell.execute_reply": "2022-01-14T09:43:11.665495Z",
     "shell.execute_reply.started": "2022-01-14T05:36:23.143011Z"
    },
    "papermill": {
     "duration": 6617.230929,
     "end_time": "2022-01-14T09:43:11.665756",
     "exception": false,
     "start_time": "2022-01-14T07:52:54.434827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 07:52:54.607315: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "      1/Unknown - 3s 3s/step - loss: 14.9279 - mean_absolute_error: 3.5811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 07:52:57.995162: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19774/19774 [==============================] - 394s 20ms/step - loss: 0.9737 - mean_absolute_error: 0.7817\n",
      "Epoch 2/16\n",
      "19774/19774 [==============================] - 403s 20ms/step - loss: 0.8826 - mean_absolute_error: 0.7424\n",
      "Epoch 3/16\n",
      "19774/19774 [==============================] - 401s 20ms/step - loss: 0.8673 - mean_absolute_error: 0.7354\n",
      "Epoch 4/16\n",
      "19774/19774 [==============================] - 400s 20ms/step - loss: 0.8573 - mean_absolute_error: 0.7309\n",
      "Epoch 5/16\n",
      "19774/19774 [==============================] - 373s 19ms/step - loss: 0.8490 - mean_absolute_error: 0.7271\n",
      "Epoch 6/16\n",
      "19774/19774 [==============================] - 370s 19ms/step - loss: 0.8417 - mean_absolute_error: 0.7237\n",
      "Epoch 7/16\n",
      "19774/19774 [==============================] - 388s 20ms/step - loss: 0.8362 - mean_absolute_error: 0.7211\n",
      "Epoch 8/16\n",
      "19774/19774 [==============================] - 385s 19ms/step - loss: 0.8314 - mean_absolute_error: 0.7189\n",
      "Epoch 9/16\n",
      "19774/19774 [==============================] - 375s 19ms/step - loss: 0.8280 - mean_absolute_error: 0.7172\n",
      "Epoch 10/16\n",
      "19774/19774 [==============================] - 381s 19ms/step - loss: 0.8249 - mean_absolute_error: 0.7159\n",
      "Epoch 11/16\n",
      "19774/19774 [==============================] - 370s 19ms/step - loss: 0.8222 - mean_absolute_error: 0.7146\n",
      "Epoch 12/16\n",
      "19774/19774 [==============================] - 373s 19ms/step - loss: 0.8200 - mean_absolute_error: 0.7136\n",
      "Epoch 13/16\n",
      "19774/19774 [==============================] - 392s 20ms/step - loss: 0.8179 - mean_absolute_error: 0.7126\n",
      "Epoch 14/16\n",
      "19774/19774 [==============================] - 370s 19ms/step - loss: 0.8159 - mean_absolute_error: 0.7116\n",
      "Epoch 15/16\n",
      "19774/19774 [==============================] - 392s 20ms/step - loss: 0.8147 - mean_absolute_error: 0.7110\n",
      "Epoch 16/16\n",
      "19774/19774 [==============================] - 377s 19ms/step - loss: 0.8131 - mean_absolute_error: 0.7103\n",
      "Test MAE: 0.715\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_dataset_from_csv('train_data.csv', shuffle=True, batch_size=265)\n",
    "model.fit(train_dataset, epochs=16)\n",
    "\n",
    "test_dataset = get_dataset_from_csv('test_data.csv', batch_size=265)\n",
    "_, mae = model.evaluate(test_dataset, verbose=0)\n",
    "print(f'Test MAE: {round(mae, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06188bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T07:39:56.378054Z",
     "iopub.status.busy": "2022-01-14T07:39:56.377413Z",
     "iopub.status.idle": "2022-01-14T07:39:56.382316Z",
     "shell.execute_reply": "2022-01-14T07:39:56.381490Z",
     "shell.execute_reply.started": "2022-01-14T07:39:56.378014Z"
    },
    "papermill": {
     "duration": 29.698466,
     "end_time": "2022-01-14T09:44:11.298005",
     "exception": false,
     "start_time": "2022-01-14T09:43:41.599539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b70affc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-14T09:45:10.325529Z",
     "iopub.status.busy": "2022-01-14T09:45:10.324974Z",
     "iopub.status.idle": "2022-01-14T09:45:10.376468Z",
     "shell.execute_reply": "2022-01-14T09:45:10.376888Z"
    },
    "papermill": {
     "duration": 29.330854,
     "end_time": "2022-01-14T09:45:10.377041",
     "exception": false,
     "start_time": "2022-01-14T09:44:41.046187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/netflix-prize-data/combined_data_3.txt\n",
      "/kaggle/input/netflix-prize-data/movie_titles.csv\n",
      "/kaggle/input/netflix-prize-data/combined_data_4.txt\n",
      "/kaggle/input/netflix-prize-data/combined_data_1.txt\n",
      "/kaggle/input/netflix-prize-data/README\n",
      "/kaggle/input/netflix-prize-data/probe.txt\n",
      "/kaggle/input/netflix-prize-data/combined_data_2.txt\n",
      "/kaggle/input/netflix-prize-data/qualifying.txt\n",
      "/kaggle/input/amazon-ratings/ratings_Beauty.csv\n",
      "/kaggle/input/netflix-ratings/netflix_ratings.parquet\n",
      "/kaggle/input/netflixratings/netflix_ratings_sampled01.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dbcb571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T09:46:09.456570Z",
     "iopub.status.busy": "2022-01-14T09:46:09.456043Z",
     "iopub.status.idle": "2022-01-14T09:46:12.409100Z",
     "shell.execute_reply": "2022-01-14T09:46:12.408522Z"
    },
    "papermill": {
     "duration": 32.633463,
     "end_time": "2022-01-14T09:46:12.409227",
     "exception": false,
     "start_time": "2022-01-14T09:45:39.775764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A39HTATAQ9V7YF</td>\n",
       "      <td>0205616461</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1369699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3JM6GV9MNOF9X</td>\n",
       "      <td>0558925278</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1355443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1Z513UWSAAO0F</td>\n",
       "      <td>0558925278</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1404691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1WMRR494NWEWV</td>\n",
       "      <td>0733001998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1382572800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3IAAVS479H7M7</td>\n",
       "      <td>0737104473</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1274227200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UserId   ProductId  Rating   Timestamp\n",
       "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
       "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
       "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
       "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
       "4  A3IAAVS479H7M7  0737104473     1.0  1274227200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/amazon-ratings/ratings_Beauty.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cae2d9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T09:47:11.648524Z",
     "iopub.status.busy": "2022-01-14T09:47:11.647550Z",
     "iopub.status.idle": "2022-01-14T09:47:16.923450Z",
     "shell.execute_reply": "2022-01-14T09:47:16.924103Z"
    },
    "papermill": {
     "duration": 34.865785,
     "end_time": "2022-01-14T09:47:16.924257",
     "exception": false,
     "start_time": "2022-01-14T09:46:42.058472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserId</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A00125322X21CGQBJ30S9</th>\n",
       "      <th>1365379200</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00262022JQPXX5SXEVJR</th>\n",
       "      <th>1397520000</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00370223FX3K9TUF1QCL</th>\n",
       "      <th>1384300800</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0038640S18JE5Y497U6</th>\n",
       "      <th>1376524800</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00414041RD0BXM6WK0GX</th>\n",
       "      <th>1405296000</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZVT7PFUPM8D</th>\n",
       "      <th>1402444800</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZWJ3LICUEKJ</th>\n",
       "      <th>1361577600</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZYW4YOE1B6E</th>\n",
       "      <th>1386633600</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZZLM1E5JJ8C</th>\n",
       "      <th>1376697600</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZZMSZI9LKE6</th>\n",
       "      <th>1361232000</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243950 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ProductId\n",
       "UserId                Timestamp            \n",
       "A00125322X21CGQBJ30S9 1365379200          2\n",
       "A00262022JQPXX5SXEVJR 1397520000          4\n",
       "A00370223FX3K9TUF1QCL 1384300800          2\n",
       "A0038640S18JE5Y497U6  1376524800          4\n",
       "A00414041RD0BXM6WK0GX 1405296000          7\n",
       "...                                     ...\n",
       "AZZVT7PFUPM8D         1402444800          3\n",
       "AZZWJ3LICUEKJ         1361577600          4\n",
       "AZZYW4YOE1B6E         1386633600          2\n",
       "AZZZLM1E5JJ8C         1376697600          4\n",
       "AZZZMSZI9LKE6         1361232000          2\n",
       "\n",
       "[243950 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df.drop_duplicates().groupby(['UserId', 'Timestamp'])[['ProductId']].count()\n",
    "tmp.loc[tmp['ProductId'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7844de1",
   "metadata": {
    "papermill": {
     "duration": 29.376563,
     "end_time": "2022-01-14T09:48:16.099077",
     "exception": false,
     "start_time": "2022-01-14T09:47:46.722514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7146.816545,
   "end_time": "2022-01-14T09:48:48.939699",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-14T07:49:42.123154",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
