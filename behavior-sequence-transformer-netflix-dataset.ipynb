{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Behavior Sequence Transformer\n\nusing Transformer to capture the sequential signals underlying users' behavior sequences  \n\n### References\n- https://arxiv.org/pdf/1905.06874.pdf  \n- https://www.kaggle.com/laowingkin/netflix-movie-recommendation","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup\n\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:24:58.973756Z","iopub.execute_input":"2022-01-14T05:24:58.974606Z","iopub.status.idle":"2022-01-14T05:25:04.256744Z","shell.execute_reply.started":"2022-01-14T05:24:58.974534Z","shell.execute_reply":"2022-01-14T05:25:04.255969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"# df1 = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['cust_id', 'rating', 'timestamp'])\n# df1['rating'] = df1['rating'].astype(float)\n# df1.head()\n\n# df2 = pd.read_csv('../input/netflix-prize-data/combined_data_2.txt', \n#                   header=None, names=['cust_id', 'rating', 'timestamp'])\n# df2['rating'] = df2['rating'].astype(float)\n# df1 = pd.concat([df1, df2])\n# print(df1.shape)\n# del df2\n\n# df3 = pd.read_csv('../input/netflix-prize-data/combined_data_3.txt', \n#                   header=None, names=['cust_id', 'rating', 'timestamp'])\n# df3['rating'] = df3['rating'].astype(float)\n# df1 = pd.concat([df1, df3])\n# print(df1.shape)\n# del df3\n\n# df4 = pd.read_csv('../input/netflix-prize-data/combined_data_4.txt', \n#                   header=None, names=['cust_id', 'rating', 'timestamp'])\n# df4['rating'] = df4['rating'].astype(float)\n# df1 = pd.concat([df1, df4])\n# print(df1.shape)\n# del df4","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df1.to_parquet('/kaggle/working/ratings.parquet')\n# df1 = pd.read_parquet('../input/netflix-ratings/netflix_ratings.parquet')\n# df1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_nan = df1.loc[df1['rating'].isna()].reset_index().drop(['rating', 'timestamp'], axis=1)\n# df_nan['next'] = df_nan['index'].shift(-1).fillna(df1.index[-1]+1).astype(int)\n# df_nan['movie_id'] = df_nan['cust_id'].str[:-1].astype(int)\n# df_nan.drop('cust_id', axis=1, inplace=True)\n\n# movie_ids = np.full((1, df1.shape[0]), 0)\n# for i, j, k in tqdm(df_nan[['index', 'next', 'movie_id']].values):\n#     movie_ids[0, i+1:j] = k\n# df1['movie_id'] = movie_ids[0]\n\n# df2 = df1.loc[~df1['rating'].isna()]\n# del df1, df_nan","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random_selection = np.random.rand(len(df2.index)) <= 0.5\n# df3 = df2[random_selection]\n# del df2\n# df_movie_summary = df3.groupby('movie_id')[['rating']].count()\n# df_user_summary = df3.groupby('cust_id')[['rating']].count()\n# drop_movie_list = df_movie_summary.loc[df_movie_summary['rating'] < 100].index\n# drop_cust_list = df_user_summary.loc[df_user_summary['rating'] < 100].index\n\n# df3 = df3[~df3['movie_id'].isin(drop_movie_list)]\n# df3 = df3[~df3['cust_id'].isin(drop_cust_list)]\n\n\ndf3 = pd.read_parquet('../input/netflixratings/netflix_ratings_sampled01.parquet')\ndf3['cust_id'] = df3['cust_id'].apply(lambda x: f'cust_{x}')\ndf3['movie_id'] = df3['movie_id'].apply(lambda x: f'movie_{x}')\n# print('here')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-14T05:30:52.573893Z","iopub.execute_input":"2022-01-14T05:30:52.574182Z","iopub.status.idle":"2022-01-14T05:31:01.822269Z","shell.execute_reply.started":"2022-01-14T05:30:52.574149Z","shell.execute_reply":"2022-01-14T05:31:01.821316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_group = df3.sort_values('timestamp').groupby('cust_id')\ndf3[['cust_id', 'movie_id']] = df3[['cust_id', 'movie_id']].astype('string')\nCATEGORICAL_FEATURES_WITH_VOCABULARY = {\n    'cust_id': list(df3.cust_id.unique()), \n    'movie_id': list(df3.movie_id.unique())\n}\ndel df3","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:31:01.824233Z","iopub.execute_input":"2022-01-14T05:31:01.82452Z","iopub.status.idle":"2022-01-14T05:31:25.333459Z","shell.execute_reply.started":"2022-01-14T05:31:01.82448Z","shell.execute_reply":"2022-01-14T05:31:25.332627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_data = pd.DataFrame(data={\n    'cust_id': list(ratings_group.groups.keys()), \n    'movie_ids': list(ratings_group.movie_id.apply(list)), \n    'ratings': list(ratings_group.rating.apply(list)),\n    'timestamps': list(ratings_group.timestamp.apply(list))\n})\n\nratings_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:31:25.334996Z","iopub.execute_input":"2022-01-14T05:31:25.335295Z","iopub.status.idle":"2022-01-14T05:31:50.808147Z","shell.execute_reply.started":"2022-01-14T05:31:25.335256Z","shell.execute_reply":"2022-01-14T05:31:50.807458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_length = 8\nstep_size = 1\n\ndef create_sequences(values, window_size, step_size):\n    sequences = []\n    start_index = 0\n    \n    while len(values[start_index:]) >= window_size:\n        end_index = start_index + window_size\n        seq = values[start_index:end_index]\n        sequences.append(seq)\n        start_index += step_size\n    return sequences\n\nratings_data.movie_ids = ratings_data.movie_ids.apply(\n    lambda ids: create_sequences(ids, sequence_length, step_size)\n)\nratings_data.ratings = ratings_data.ratings.apply(\n    lambda ids: create_sequences(ids, sequence_length, step_size)\n)\n\nratings_data.drop('timestamps', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:33:36.569428Z","iopub.execute_input":"2022-01-14T05:33:36.569786Z","iopub.status.idle":"2022-01-14T05:34:02.859142Z","shell.execute_reply.started":"2022-01-14T05:33:36.569749Z","shell.execute_reply":"2022-01-14T05:34:02.858392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_data_movies = ratings_data[['cust_id', 'movie_ids']]\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t.explode('movie_ids', ignore_index=True)\nratings_data_rating = ratings_data[['ratings']]\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t.explode('ratings', ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:34:02.860656Z","iopub.execute_input":"2022-01-14T05:34:02.861037Z","iopub.status.idle":"2022-01-14T05:34:05.439571Z","shell.execute_reply.started":"2022-01-14T05:34:02.861Z","shell.execute_reply":"2022-01-14T05:34:05.438518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del ratings_data\nratings_data_transformed = \\\n\t\tpd.concat([ratings_data_movies, ratings_data_rating], axis=1).dropna()\n# del ratings_data_movies, ratings_data_rating","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:34:27.461971Z","iopub.execute_input":"2022-01-14T05:34:27.462279Z","iopub.status.idle":"2022-01-14T05:34:31.572887Z","shell.execute_reply.started":"2022-01-14T05:34:27.462245Z","shell.execute_reply":"2022-01-14T05:34:31.572112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_data_transformed.movie_ids = \\\n\t\tratings_data_transformed.movie_ids\\\n\t\t.apply(lambda x: ','.join(x))\nratings_data_transformed.ratings = \\\n\t\tratings_data_transformed.ratings\\\n\t\t.apply(lambda x: ','.join([str(v) for v in x]))\nratings_data_transformed.rename(\n    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n    inplace=True,\n)\nprint('here3')\n\nrandom_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.8\ntrain_data = ratings_data_transformed[random_selection]\ntest_data = ratings_data_transformed[~random_selection]\n\n# train_data.to_parquet('/kaggle/working/train_data.parquet')\n# test_data.to_parquet('/kaggle/working/test_data.parquet')\ntrain_data.to_csv(\"/kaggle/working/train_data.csv\", index=False, sep=\"|\", header=False)\ntest_data.to_csv(\"/kaggle/working/test_data.csv\", index=False, sep=\"|\", header=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:34:39.244597Z","iopub.execute_input":"2022-01-14T05:34:39.245081Z","iopub.status.idle":"2022-01-14T05:35:51.57348Z","shell.execute_reply.started":"2022-01-14T05:34:39.245043Z","shell.execute_reply":"2022-01-14T05:35:51.572599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CSV_HEADER = list(ratings_data_transformed.columns)\nCSV_HEADER = ['cust_id', 'sequence_movie_ids', 'sequence_ratings']\n\ndef get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n    def process(features):\n        movie_ids_string = features['sequence_movie_ids']\n        sequence_movie_ids = tf.strings.split(movie_ids_string, ',').to_tensor()\n        \n        features['target_movie_id'] = sequence_movie_ids[:, -1]\n        features['sequence_movie_ids'] = sequence_movie_ids[:, :-1]\n        \n        ratings_string = features['sequence_ratings']\n        sequence_ratings = tf.strings.to_number(\n            tf.strings.split(ratings_string, ','), tf.dtypes.float32\n        ).to_tensor()\n        \n        target = sequence_ratings[:, -1]\n        features['sequence_ratings'] = sequence_ratings[:, :-1]\n        return features, target\n    \n    dataset = tf.data.experimental.make_csv_dataset(\n        csv_file_path, \n        batch_size=batch_size, \n        column_names=CSV_HEADER, \n        num_epochs=1,\n        header=False,\n        field_delim='|',\n        shuffle=shuffle\n    ).map(process)\n    \n    return dataset\n\ntrain_dataset = get_dataset_from_csv('train_data.csv', shuffle=True, batch_size=265)\n\nsequence_length = 8\ninclude_user_id = False\nhidden_units = [256, 128]\ndropout_rate = 0.1\nnum_heads = 3\n\ndef create_model_inputs():\n    return {\n        'cust_id': layers.Input(name='cust_id', shape=(1,), dtype=tf.string), \n        'sequence_movie_ids': layers.Input(name='sequence_movie_ids', \n                                           shape=(sequence_length - 1,), \n                                           dtype=tf.string),\n        'target_movie_id': layers.Input(name='target_movie_id', \n                                        shape=(1,), dtype=tf.string),\n        'sequence_ratings': layers.Input(name='sequence_ratings', \n                                         shape=(sequence_length - 1,), \n                                         dtype=tf.float32)\n    }\n\ninputs = create_model_inputs()","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:35:51.579154Z","iopub.execute_input":"2022-01-14T05:35:51.581402Z","iopub.status.idle":"2022-01-14T05:35:52.779973Z","shell.execute_reply.started":"2022-01-14T05:35:51.581361Z","shell.execute_reply":"2022-01-14T05:35:52.779179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_input_features(inputs, include_user_id=True):\n    \n    encoded_transformer_features = []\n    encoded_other_features = []\n    other_feature_names = []\n    \n    if include_user_id:\n        other_feature_names.append('cust_id')\n        \n    for feature_name in other_feature_names:\n        # string input values -> integer indices\n        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n        # nlp의 tokenizer와 비슷한 역할\n        idx = StringLookup(vocabulary=vocabulary, mask_token=None, \n                           num_oov_indices=0)(inputs[feature_name])\n        \n        embedding_dims = int(math.sqrt(len(vocabulary)))\n        embedding_encoder = layers.Embedding(\n            input_dim=len(vocabulary),\n            output_dim=embedding_dims,\n            name=f'{feature_name}_embedding'\n        )\n        \n        # convert the index values to embedding representation\n        encoded_other_features.append(embedding_encoder(idx))\n        \n    if len(encoded_other_features) == 1:\n        encoded_other_features = encoded_other_features[0]\n    else:\n        encoded_other_features = None\n        \n    ###########################################################################\n    # movie_id                               \n    ###########################################################################\n    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY['movie_id']\n    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n    movie_index_lookup = StringLookup(\n        vocabulary=movie_vocabulary,\n        mask_token=None,\n        num_oov_indices=0,\n        name='movie_index_lookup'\n    )\n    \n    movie_embedding_encoder = layers.Embedding(\n        input_dim=len(movie_vocabulary),\n        output_dim=movie_embedding_dims,\n        name='movie_embedding'\n    )\n    \n    # define a function to encode a given movie id\n    def encode_movie(movie_id):\n        # string input -> integer indices\n        movie_idx = movie_index_lookup(movie_id)\n        encoded_movie = movie_embedding_encoder(movie_idx)\n        \n        return encoded_movie\n    \n    target_movie_id = inputs['target_movie_id']\n    encoded_target_movie = encode_movie(target_movie_id)\n    \n    sequence_movie_ids = inputs['sequence_movie_ids']\n    encoded_sequence_movies = encode_movie(sequence_movie_ids)\n    \n    ###########################################################################    \n    # position embedding\n    ###########################################################################\n    position_embedding_encoder = layers.Embedding(\n        input_dim=sequence_length, \n        output_dim=movie_embedding_dims, \n        name='poisition_embedding'\n    )\n    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n    encoded_positions = position_embedding_encoder(positions)\n    \n    ###########################################################################\n    # ratings\n    ###########################################################################\n    # shape (None, 7) -> shape (None, 7, 1)  \n    sequence_ratings = tf.expand_dims(inputs['sequence_ratings'], -1)\n    \n    \n    ###########################################################################\n    # inner product of movie id sequence + encoded position & sequence_rating\n    ###########################################################################\n    \n    # encoded_sequence_movies shape (7,114)\n    # encoded_positions shape (7,114)\n    # sequence_ratings shape (None, 7, 1)\n    # encoded_sequence_movies_with_position_and_rating shape (None, 7, 114)\n    \n    encoded_sequence_movies_with_position_and_rating = layers.Multiply()(\n        [(encoded_sequence_movies + encoded_positions), sequence_ratings]\n    )\n    \n    \n    # unstack -> (None, 7, 114)에서 114씩 encoded_movie로 분리됨\n    for encoded_movie in tf.unstack(encoded_sequence_movies_with_position_and_rating, axis=1):\n        # encoded_movie shape(None, 114) -> (None, 1, 114)\n        encoded_transformer_features.append(tf.expand_dims(encoded_movie, 1))\n    \n    encoded_transformer_features.append(encoded_target_movie)\n    encoded_transformer_features = layers.concatenate(encoded_transformer_features, axis=1)\n    \n    return encoded_transformer_features, encoded_other_features","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:35:52.781481Z","iopub.execute_input":"2022-01-14T05:35:52.78202Z","iopub.status.idle":"2022-01-14T05:35:52.798171Z","shell.execute_reply.started":"2022-01-14T05:35:52.781976Z","shell.execute_reply":"2022-01-14T05:35:52.797132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_features, other_features = encode_input_features(\n    inputs, include_user_id\n)\n\n# create a multi-headed attention layer\n# params: (target, source)\nattention_output = layers.MultiHeadAttention(\n    num_heads=num_heads, \n    key_dim=transformer_features.shape[2], \n    dropout=dropout_rate\n)(transformer_features, transformer_features)\n\n# transformer block\nattention_output = layers.Dropout(dropout_rate)(attention_output)\nx1 = layers.Add()([transformer_features, attention_output])\nx1 = layers.LayerNormalization()(x1)\nx2 = layers.LeakyReLU()(x1)\n# 왜 shape[-1]인가\nx2 = layers.Dense(units=x2.shape[-1])(x2)\nx2 = layers.Dropout(dropout_rate)(x2)\ntransformer_features = layers.Add()([x1, x2])\ntransformer_features = layers.LayerNormalization()(transformer_features)\nfeatures = layers.Flatten()(transformer_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:35:52.801138Z","iopub.execute_input":"2022-01-14T05:35:52.801523Z","iopub.status.idle":"2022-01-14T05:35:53.065975Z","shell.execute_reply.started":"2022-01-14T05:35:52.801481Z","shell.execute_reply":"2022-01-14T05:35:53.065212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if other_features is not None:\n    features = layers.concatenate(\n        [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n    )\n\nfor num_units in hidden_units:\n    features = layers.Dense(num_units)(features)\n    features = layers.BatchNormalization()(features)\n    features = layers.LeakyReLU()(features)\n    features = layers.Dropout(dropout_rate)(features)\n    \noutputs = layers.Dense(units=1)(features)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.01),\n    loss=tf.keras.losses.MeanSquaredError(),\n    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:35:53.067137Z","iopub.execute_input":"2022-01-14T05:35:53.067389Z","iopub.status.idle":"2022-01-14T05:35:53.167498Z","shell.execute_reply.started":"2022-01-14T05:35:53.067356Z","shell.execute_reply":"2022-01-14T05:35:53.166693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = get_dataset_from_csv('train_data.csv', shuffle=True, batch_size=265)\nmodel.fit(train_dataset, epochs=16)\n\ntest_dataset = get_dataset_from_csv('test_data.csv', batch_size=265)\n_, mae = model.evaluate(test_dataset, verbose=0)\nprint(f'Test MAE: {round(mae, 3)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-14T05:36:23.142773Z","iopub.execute_input":"2022-01-14T05:36:23.143041Z","iopub.status.idle":"2022-01-14T07:20:42.810933Z","shell.execute_reply.started":"2022-01-14T05:36:23.143011Z","shell.execute_reply":"2022-01-14T07:20:42.810093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-01-14T07:39:56.377413Z","iopub.execute_input":"2022-01-14T07:39:56.378054Z","iopub.status.idle":"2022-01-14T07:39:56.382316Z","shell.execute_reply.started":"2022-01-14T07:39:56.378014Z","shell.execute_reply":"2022-01-14T07:39:56.38149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/amazon-ratings/ratings_Beauty.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = df.drop_duplicates().groupby(['UserId', 'Timestamp'])[['ProductId']].count()\ntmp.loc[tmp['ProductId'] >= 2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}